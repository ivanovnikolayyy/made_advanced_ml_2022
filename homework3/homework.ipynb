{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673deb6b",
   "metadata": {},
   "source": [
    "# Продвинутое машинное обучение: Домашнее задание 3\n",
    "\n",
    "Третье домашнее задание посвящено достаточно простой, но, надеюсь, интересной задаче, в которой потребуется творчески применить методы сэмплирования. Как всегда, любые комментарии, новые идеи и рассуждения на тему категорически приветствуются.\n",
    "\n",
    "В этом небольшом домашнем задании мы попробуем улучшить метод Шерлока Холмса. Как известно, в рассказе The Adventure of the Dancing Men великий сыщик расшифровал загадочные письмена, которые выглядели примерно так:\n",
    "\n",
    "![](img/dancing_men.png)\n",
    "\n",
    "Пользовался он для этого так называемым частотным методом: смотрел, какие буквы чаще встречаются в зашифрованных текстах, и пытался подставить буквы в соответствии с частотной таблицей: E — самая частая и так далее.\n",
    "\n",
    "В этом задании мы будем разрабатывать более современный и продвинутый вариант такого частотного метода. В качестве корпусов текстов для подсчётов частот можете взять что угодно, но для удобства вот вам “Война и мир” по-русски и по-английски:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412100b5",
   "metadata": {},
   "source": [
    "## 1. Baseline\n",
    "\n",
    "Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cc31155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from typing import List, Dict, Callable\n",
    "from collections import Counter\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_texts(data_path: str) -> List[str]:\n",
    "    with open(data_path, \"r\") as input_stream:\n",
    "        texts = input_stream.read().splitlines()\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "def preprocess_rus(text: str):\n",
    "    text = re.sub(r'[^а-ё ]+', '', text.lower())\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_eng(text: str):\n",
    "    text = re.sub(r'[^a-z ]+', '', text.lower())\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_texts(\n",
    "    texts: List[str], first_sentence: str = '', last_sentence: str = '', alphabet: str = 'rus'\n",
    ") -> str:\n",
    "\n",
    "    if alphabet == 'rus':\n",
    "        texts = [preprocess_rus(text) for text in texts]\n",
    "    elif alphabet == 'eng':\n",
    "        texts = [preprocess_eng(text) for text in texts]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    preprocessed = [text for text in texts if text]\n",
    "\n",
    "    if first_sentence:\n",
    "        index_first = texts.index(first_sentence)\n",
    "        texts = texts[index_first:]\n",
    "\n",
    "    if last_sentence:\n",
    "        index_end = texts.index(last_sentence)\n",
    "        texts = texts[:index_end]\n",
    "    \n",
    "    texts = ' '.join(texts)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e67f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/AnnaKarenina.txt\"\n",
    "first_sentence = \"все счастливые семьи похожи друг на друга каждая несчастливая семья несчастлива посвоему\"\n",
    "last_sentence = \"примечания\"\n",
    "\n",
    "texts = read_texts(data_path)\n",
    "corpus = preprocess_texts(texts, first_sentence, last_sentence, 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837ffad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"data/WarAndPeace.txt\"\n",
    "# first_sentence = \"часть первая\"\n",
    "# last_sentence = \"\"\n",
    "\n",
    "# texts = read_texts(data_path)\n",
    "# corpus = preprocess_texts(texts, first_sentence, last_sentence, 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3f1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"data/WarAndPeaceEng.txt\"\n",
    "# first_sentence = \"well prince so genoa and lucca are now just family estates of the\"\n",
    "# last_sentence = \"end of the project gutenberg ebook of war and peace by leo tolstoy\"\n",
    "\n",
    "# texts = read_texts(data_path)\n",
    "# corpus = preprocess_texts(texts, first_sentence, last_sentence, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c5ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_chars(text: str) -> List[chr]:\n",
    "    \"\"\"Возвращает список биграм в порядке уменьшения частоты встречаемости в тексте text\"\"\"\n",
    "    \n",
    "    sorted_chars = [item[0] for item in Counter(text).most_common()]\n",
    "    \n",
    "    return sorted_chars\n",
    "\n",
    "\n",
    "def encode_chars(text: str, encoding: Dict[chr, chr]) -> str:\n",
    "    \"\"\"Кодирует текст text при помощи замены символов encoding\"\"\"\n",
    "    \n",
    "    encoded_text = ''.join(map(lambda x: encoding.get(x, ' '), text))\n",
    "    return encoded_text\n",
    "\n",
    "\n",
    "def encode_decode_chars(text: str, corpus: str) -> Dict[str, str]:\n",
    "    \"\"\"Кодирует текст text, а затем декодирует при помощи корпуса corpus, используя частотный метод по символам\"\"\"\n",
    "    \n",
    "    # посчитаем частоты n-грамм в корпусе\n",
    "    sorted_corpus_chars = get_sorted_chars(corpus)\n",
    "    \n",
    "    # случайным образом переставим буквы в алфавите\n",
    "    shuffling = sorted_corpus_chars.copy()\n",
    "    random.shuffle(shuffling)\n",
    "    encoding = dict(zip(sorted_corpus_chars, shuffling))\n",
    "    \n",
    "    # закодируем текст случайной перестановкой букв\n",
    "    encoded_text = encode_chars(text, encoding)\n",
    "    \n",
    "    # посчитаем частоты n-грамм в закодированном тексте\n",
    "    sorted_encoded_text_chars = get_sorted_chars(encoded_text)\n",
    "    \n",
    "    # декодируем текст\n",
    "    inverse_encoding = dict(zip(sorted_encoded_text_chars, sorted_corpus_chars))\n",
    "    decoded_text = encode_chars(encoded_text, inverse_encoding)\n",
    "    \n",
    "    # результат в удобном виде\n",
    "    result = {\n",
    "        \"original_text\": text,\n",
    "        \"encoded_text\": encoded_text,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1d4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    Есть люди, которые, встречая своего счастливого в чем бы то ни было соперника,\n",
    "    готовы сейчас же отвернуться от всего хорошего, что есть в нем, и видеть в нем одно дурное;\n",
    "    есть люди, которые, напротив, более всего желают найти в этом счастливом сопернике те качества,\n",
    "    которыми он победил их, и ищут в нем со щемящею болью в сердце одного хорошего.\n",
    "\"\"\"\n",
    "\n",
    "text = preprocess_rus(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39e5fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_text': 'есть люди которые встречая своего счастливого в чем бы то ни было соперника готовы сейчас же отвернуться от всего хорошего что есть в нем и видеть в нем одно дурное есть люди которые напротив более всего желают найти в этом счастливом сопернике те качества которыми он победил их и ищут в нем со щемящею болью в сердце одного хорошего',\n",
       " 'encoded_text': 'шбсрйжчёяйьцсцтишйыбстшгюмйбыцшэцйбгюбсжяыцэцйыйгшъй ийсцйеяй ижцйбцуштеяьюйэцсцыийбшнгюбйпшйцсыштеосрбмйцсйыбшэцйфцтцлшэцйгсцйшбсрйыйешъйяйыяёшсрйыйешъйцёецйёотецшйшбсрйжчёяйьцсцтишйеюутцсяый цжшшйыбшэцйпшжючсйеюнсяйыйхсцъйбгюбсжяыцъйбцуштеяьшйсшйьюгшбсыюйьцсцтиъяйцейуц шёяжйяфйяйядосйыйешъйбцйдшъмдшчй цжрчйыйбштёзшйцёецэцйфцтцлшэц',\n",
       " 'decoded_text': 'енап рыкт яоаолье иналеувч ниоедо нувнартиодо и уем гь ао ст гьро нобелстяв доаоиь нешувн хе оаиелсзапнч оа инедо жолоюедо уао енап и сем т иткеап и сем оксо кзлсое енап рыкт яоаолье свблоати горее инедо хервыа свшат и эаом нувнартиом нобелстяе ае явуенаив яоаольмт ос богектр тж т тйза и сем но йемчйеы горпы и нелкще оксодо жолоюедо'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_chars_result = encode_decode_chars(text, corpus)\n",
    "decode_chars_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16984795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(text1, text2):\n",
    "    \"\"\"Процент корректно расшифрованных букв\"\"\"\n",
    "    \n",
    "    correct = sum([a == b for a, b in zip(text1, text2)])\n",
    "    score = correct / len(text1)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fbc4bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4221556886227545"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    decode_chars_result[\"original_text\"],\n",
    "    decode_chars_result[\"decoded_text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd2a0b",
   "metadata": {},
   "source": [
    "## 2. Bigrams\n",
    "\n",
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac547e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text: str) -> List[str]:\n",
    "    \"\"\"Возвращает список биграм в произвольном порядке\"\"\"\n",
    "    \n",
    "    return [text[i] + text[i + 1] for i in range(0, len(text) - 1, 2)]\n",
    "\n",
    "\n",
    "def get_sorted_bigrams(text: str) -> List[chr]:\n",
    "    \"\"\"Возвращает список биграм в порядке уменьшения частоты встречаемости в тексте text\"\"\"\n",
    "    \n",
    "    bigrams = get_bigrams(text)\n",
    "    sorted_bigrams = [item[0] for item in Counter(bigrams).most_common()]\n",
    "    \n",
    "    return sorted_bigrams\n",
    "\n",
    "\n",
    "def encode_bigrams(text: str, encoding: Dict[chr, chr]) -> str:\n",
    "    \"\"\"Кодирует текст text при помощи замены биграм encoding\"\"\"\n",
    "    \n",
    "    bigrams = get_bigrams(text)\n",
    "    encoded_text = ''.join(map(lambda x: encoding.get(x, ' '), bigrams))\n",
    "    \n",
    "    return encoded_text\n",
    "\n",
    "\n",
    "def encode_decode_bigrams(text: str, corpus: str) -> Dict[str, str]:\n",
    "    \"\"\"Кодирует текст text, а затем декодирует при помощи корпуса corpus, используя частотный метод по биграмам\"\"\"\n",
    "    \n",
    "    # посчитаем частоты биграм в корпусе\n",
    "    sorted_corpus_bigrams = get_sorted_bigrams(corpus)\n",
    "    \n",
    "    # случайным образом переставим биграмы\n",
    "    shuffling = sorted_corpus_bigrams.copy()\n",
    "    random.shuffle(shuffling)\n",
    "    encoding = dict(zip(sorted_corpus_bigrams, shuffling))\n",
    "    \n",
    "    # закодируем текст случайной перестановкой биграм\n",
    "    encoded_text = encode_bigrams(text, encoding)\n",
    "    \n",
    "    # посчитаем частоты биграм в закодированном тексте\n",
    "    sorted_encoded_text_bigrams = get_sorted_bigrams(encoded_text)\n",
    "    \n",
    "    # декодируем текст\n",
    "    inverse_encoding = dict(zip(sorted_encoded_text_bigrams, sorted_corpus_bigrams))\n",
    "    decoded_text = encode_bigrams(encoded_text, inverse_encoding)\n",
    "    \n",
    "    # результат в удобном виде\n",
    "    result = {\n",
    "        \"original_text\": text,\n",
    "        \"encoded_text\": encoded_text,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed21f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_text': 'есть люди которые встречая своего счастливого в чем бы то ни было соперника готовы сейчас же отвернуться от всего хорошего что есть в нем и видеть в нем одно дурное есть люди которые напротив более всего желают найти в этом счастливом сопернике те качества которыми он победил их и ищут в нем со щемящею болью в сердце одного хорошего',\n",
       " 'encoded_text': ' слзкцьэтэтмдыудбкурыжлюбгдз члднд гуубиьиоунд ттудцхмдшндтстзоонд ькфюрхгпюусдыщодзчмзтыщмшьцянязвълзмяьцсшурлдндзърыдбусёкдыикычпо тшндцтэрдвчлзнрчсаеьцчандбъюрнжикычпохсея япьншижчсфорыэ  тйдчобкурлдндмшзьоччсхуэ нреидыдц гуубиьиюкдзосязтсбцдшбкцытуычер япьншаютэфувегруе щзрадтэсььхнрчсаедзндбыорбырюйджлрю тгодмельцчаоундзърыдбус',\n",
       " 'decoded_text': ' чтотьпре оса ло п оелн  ми ор ио ално к  оно  нгоя вапоо каза ео оллеков ернаа лии ретатела сом тскток  сет о ио ниь  днаога овстл  ниля е вичттоне вен сй о декоановстл т ривом  бсе вы ь от ну  у п о ио ладаак вазотнебыа я ално к  веи  з ткаатпо пингостжевом  боде емит ртиесегдое  ласне вени о рольрорау всра ню обче сй оно ниь  дна'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_bigrams_result = encode_decode_bigrams(text, corpus)\n",
    "decode_bigrams_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbdce603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17365269461077845"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    decode_bigrams_result[\"original_text\"],\n",
    "    decode_bigrams_result[\"decoded_text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329af951",
   "metadata": {},
   "source": [
    "## 3. MCMC on bigrams\n",
    "\n",
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2562ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_freq(text: str) -> Dict[str, float]:\n",
    "    bigrams = get_bigrams(text)\n",
    "    bigrams_freq = dict(Counter(bigrams).most_common())\n",
    "    bigrams_freq = {k: v / len(bigrams) for k, v in bigrams_freq.items()}\n",
    "    \n",
    "    return bigrams_freq\n",
    "\n",
    "\n",
    "def get_log_proba(decoded_text: str, corpus_bigrams_freq: Dict[str, int]) -> float:\n",
    "    min_freq = min(corpus_bigrams_freq.values())\n",
    "    log_proba = 0\n",
    "    \n",
    "    for i in range(0, len(decoded_text) - 1):\n",
    "        bigram = text[i] + decoded_text[i + 1]\n",
    "        proba = corpus_bigrams_freq.get(bigram, min_freq)\n",
    "        log_proba += np.log(proba)\n",
    "    \n",
    "    return log_proba\n",
    "\n",
    "\n",
    "def encode_decode_mmc(text: str, corpus: str, num_trials: int = 1000, num_steps: int = 10) -> Dict[str, str]:\n",
    "    # получим отсортированные по убыванию частоты биграмы корпуса\n",
    "    sorted_corpus_bigrams = get_sorted_bigrams(corpus)\n",
    "    \n",
    "    # случайным образом переставим биграмы\n",
    "    shuffling = sorted_corpus_bigrams.copy()\n",
    "    random.shuffle(shuffling)\n",
    "    encoding = dict(zip(sorted_corpus_bigrams, shuffling))\n",
    "    \n",
    "    # закодируем текст случайной перестановкой биграм\n",
    "    encoded_text = encode_bigrams(text, encoding)\n",
    "    \n",
    "    # получим отсортированные по убыванию частоты биграмы закодированного текста\n",
    "    sorted_encoded_text_bigrams = get_sorted_bigrams(encoded_text)\n",
    "    \n",
    "    # получим частоты биграм в корпусе\n",
    "    corpus_bigrams_freq = get_bigrams_freq(corpus)\n",
    "    \n",
    "    best_log_proba = float(\"-inf\")\n",
    "    best_inverse_encoding = None\n",
    "    \n",
    "    for trial in tqdm.tqdm(range(num_trials)):\n",
    "\n",
    "        inverse_encoding = dict(zip(sorted_encoded_text_bigrams, sorted_corpus_bigrams))\n",
    "        decoded_text = encode_bigrams(encoded_text, inverse_encoding)\n",
    "        log_proba = get_log_proba(decoded_text, corpus_bigrams_freq)\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            new_text_bigrams = sorted_encoded_text_bigrams.copy()\n",
    "            \n",
    "            i, j = np.random.choice(len(sorted_encoded_text_bigrams), size=2, replace=False)\n",
    "            new_text_bigrams[i], new_text_bigrams[j] = new_text_bigrams[j], new_text_bigrams[i]\n",
    "            \n",
    "            new_inverse_encoding = dict(zip(new_text_bigrams, sorted_corpus_bigrams))\n",
    "            new_decoded_text = encode_bigrams(encoded_text, new_inverse_encoding)\n",
    "            new_log_proba = get_log_proba(new_decoded_text, corpus_bigrams_freq)\n",
    "            \n",
    "            p = np.exp(new_log_proba - log_proba)\n",
    "            \n",
    "            if p > random.random():\n",
    "                log_proba = new_log_proba\n",
    "                inverse_encoding = new_inverse_encoding\n",
    "            \n",
    "        if log_proba > best_log_proba:\n",
    "            max_log_proba = log_proba\n",
    "            best_inverse_encoding = inverse_encoding\n",
    "    \n",
    "    decoded = encode_bigrams(encoded_text, best_inverse_encoding)\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d50842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:07<00:00, 74.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' чтотьпре оса ло п оелн  ми ор ио ално к  оно  нгоя вапоо каза ео оллеков ернаа лии ретатела сом тскток  сет о ио ниен днаога овстл  ниля е вичттоне вь  сй о декоановстл т ривом  бсе вы енот ну  у п о ио ладаак вазотнебыа я ално к  веи  з ткаатпо пингостжевом  боде емит ртиесегдое  ласне вь и о рольрорау всра ню обче сй оно ниен дна'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_decode_mmc(text, corpus, num_trials=5000, num_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b52b98",
   "metadata": {},
   "source": [
    "## 4. Predict\n",
    "\n",
    "Расшифруйте сообщение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e825bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏\"# text2 = \"დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a1b1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_decode_mmc(text1, corpus, num_trials=5000, num_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51ea54",
   "metadata": {},
   "source": [
    "## 5*. Trigrams\n",
    "\n",
    "Бонус: а что если от биграмм перейти к триграммам (тройкам букв) или даже больше? Улучшатся ли результаты? Когда улучшатся, а когда нет? Чтобы ответить на этот вопрос эмпирически, уже может понадобиться погенерировать много тестовых перестановок и последить за метриками, глазами может быть и не видно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63d9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae6dc33",
   "metadata": {},
   "source": [
    "## 6*. Assume\n",
    "Какие вы можете придумать применения для этой модели? Пляшущие человечки ведь не так часто встречаются в жизни (хотя встречаются! и это самое потрясающее во всей этой истории, но об этом я расскажу потом)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d95288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-mipt",
   "language": "python",
   "name": "ml-mipt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
